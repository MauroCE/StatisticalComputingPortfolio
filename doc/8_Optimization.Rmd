---
title: "Optimization for Classification"
output: html_document
---
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{algorithm2e}
\usepackage{bbm}
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}
\newcommand{\def}{\overset{\text{def}}{:=}}
\newcommand{\lop}{\mathcal{L}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\vx}{\vect{x}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vz}{\vect{z}}
\newcommand{\ve}{\vect{e}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vr}{\vect{r}}
\newcommand{\vphi}{\vect{\phi}}
\newcommand{\vf}{\vect{f}}
\newcommand{\vY}{\vect{Y}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vw}{\vect{w}}
\newcommand{\thetahatis}{\widehat{\theta}^{(s)}_i}
\newcommand{\thetahat}[1]{\widehat{\theta}^{(#1)}_i}
\newcommand{\vm}{\vect{m}}
\newcommand{\redmath}[1]{\mathbin{\textcolor{red}{\vect{#1}}}}
\newcommand{\redtext}[1]{\textcolor{red}{\vect{#1}}}
\newcommand{\vzero}{\vect{0}}
\newcommand{\vt}{\vect{t}}
\newcommand{\linearpredictor}{\vx_i^T\vbeta}
\newcommand{\vmu}{\vect{\mu}}
\newcommand{\vnu}{\vect{\nu}}
\newcommand{\veta}{\vect{\eta}}
\newcommand{\vbeta}{\vect{\beta}}
\newcommand{\vepsilon}{\vect{\epsilon}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\vdelta}{\vect{\delta}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\vxi}{\vect{\xi}}
\newcommand{\sigmalg}{\sigma\text{-algebra}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vlambda}{\vect{\lambda}}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\vS}{\vect{S}}
\newcommand{\sample}{\vz^{(l)}}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}
\newcommand{\sol}[1]{\vx^{(#1)}}
\newcommand{\qtext}[1]{\quad\quad \text{#1}}
\newcommand{\vtheta}{\vect{\theta}}
\newcommand{\bi}[1]{\textbf{\textit{#1}}}
\newcommand{\iid}{\overset{\text{i.i.d.}}{\sim}}
\newcommand{\uniform}{\mathcal{U}(0, 1)}
\newcommand{\qimplies}{\quad\Longrightarrow\quad}
\newcommand{\tp}{\tilde{p}}
\newcommand{\nul}{\Theta^{(0)}}
\newcommand{\alter}{\Theta^{(1)}}
\newcommand{\const}{\mathcal{Z}}
\newcommand{\tq}{\tilde{q}}
\newcommand{\tvx}{\widetilde{\vx}}
\newcommand{\vK}{\vect{K}}
\newcommand{\tr}{\tilde{r}}
\newcommand{\like}{\mathcal{L}}
\newcommand{\kl}[2]{\text{KL}(#1\,\,||\,\,#2)}
\newcommand{\logit}[1]{\log\left(\frac{#1}{1-#1}\right)}
\newcommand{\elbo}[1]{\text{elbo}(#1)}
\newcommand{\eval}{\biggr\rvert}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\vk}{\vect{k}}
\newcommand{\class}{\mathcal{C}}
\newcommand{\infor}{\mathcal{I}}
\newcommand{\variance}{\text{Var}}
\newcommand{\vfx}{\vect{f}(X)}
\newcommand{\events}{\mathcal{F}}
\newcommand{\vSigma}{\vect{\Sigma}}
\newcommand{\fls}{f_{\text{LS}}}
\newcommand{\Ebbeps}[1]{\Ebb_{\vepsilon}\left[#1\right]}
\newcommand{\vKc}{\vect{\mathcal{K}}}
\newcommand{\Var}[1]{\text{Var}\left[#1\right]}
\newcommand{\lp}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\min       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}
\newcommand{\ip}[2]{\langle #1\, , \, #2 \rangle}
\newcommand{\lpmax}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\max       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}
\newcommand{\iphi}[2]{\ip{\phi(\vx_{#1})}{\phi(\vx_{#2})}}
\newcommand{\fourmatrix}[4]{
\begin{pmatrix}
#1 & \cdots & #2 \\
\vdots & \ddots & \vdots \\
#3 & \cdots & #4
\end{pmatrix}
}
\newcommand{\Cov}[2]{\text{Cov}\left[#1, #2\right]}
\newcommand{\wls}{\vw_{\text{LS}}}
\newcommand{\fxwls}{f(\vx, \vw_{\text{LS}})}
\newcommand{\vPhi}{\vect{\Phi}}
\newcommand{\wlsr}{\vw_{\text{LS-R}}}
\newcommand{\mcal}{\mathcal{M}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\alphahat}{\widehat{\alpha}}
\newcommand{\expectation}[1]{\Ebb\left[#1\right]}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\indicator}[1]{\mathbbm{1}_{\left\{#1\right\}}}
\newcommand{\ustop}{U^{\text{stopped}}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(microbenchmark)
library(tidyverse)
```

# Utility functions

```{r}
kernel_matrix <- function(X, sigmasq, Y=NULL){
  if (is.null(Y)){
    Y <- X  # TODO: This might be risky for big matrices. How does R copy work? Maybe split the code into two?
  }
  n <- nrow(X)
  m <- nrow(Y)
  # Find three matrices above
  Xnorm <- matrix(apply(X^2, 1, sum), n, m)
  Ynorm <- matrix(apply(Y^2, 1, sum), n, m, byrow=TRUE)
  XY <- tcrossprod(X, Y)
  # TODO: In this case do we use median of XY? If we are using it for prediction
  # I think sigmasq should be found only from training set, not from testing set.
  # Therefore we should pass sigmasq as a parameter in this function, and should be calculated 
  # beforehand.
  return(exp(-(Xnorm - 2*XY + Ynorm) / (2*sigmasq)))
}

# Creates a logical one-hot encoding, assumes classes 0, .. , K-1
make_flags <- function(y){
  classes <- unique(y)
  flags <- data.frame(y=y) %>% 
            mutate(rn=row_number(), value=1) %>% 
            spread(y, value, fill=0) %>% 
            select(-rn) %>% as.matrix %>% as.logical %>%
            matrix(nrow=nrow(y)) 
  return(flags)
}
# Calculates a function (usually mean or sd) for all classes
calc_func_per_class <- function(X, y, func){
  flags <- make_flags(y)
  return(t(apply(flags, 2, function(f) apply(X[f, ], 2, func))))
}
# Calculates density of a gaussian
gaussian_density <- function(x, mu, sigma, log=FALSE, precision=FALSE){
  # If precision=TRUE, we're provided with precision matrix, which is the inverse
  # of the variance-covariance matrix
  if (!precision){
    # Cholesky, L is upper triangular
    L <- chol(sigma)
    kernel <- -0.5*crossprod(forwardsolve(t(L), x - mu))
    value  <- -length(x)*log(2*pi)/2 -sum(log(diag(L))) + kernel
  } else {
    kernel <- -0.5* t?(x-  mu) %*% (sigma %*% (x - mu))
    value  <- -length(x)*log(2*pi)/2 -log(det(L)) + kernel
  }
  if (!log){
    value <- exp(value)
  }
  return(value[[1]])
}
# Fastest known pure-R way to compute means across dimensions. See my helper Rmd with benchmarks
center_matrix <- function(A){
  return(A - rep(1, nrow(A)) %*% t(colMeans(A)))
}
```

# Binary Dataset Generation
### Settings for Data Creation
```{r}
# Settings
n1 <- 100
n2 <- 100
m1 <- c(6, 6)
m2 <- c(-1, 1)
s1 <- matrix(c(1, 0, 0, 10), nrow=2, ncol=2)
s2 <- matrix(c(1, 0, 0, 10), nrow=2, ncol=2)
```
### Data Generating Function
```{r}
generate_binary_data <- function(n1, n2, m1, s1, m2, s2){
  # x1, x2 and y for both classes (both 0,1 and -1,1 will be created for convenience)
  class1 <- mvrnorm(n1, m1, s1)
  class2 <- mvrnorm(n2, m2, s2)
  y      <- c(rep(0, n1), rep(1, n2))   # {0 , 1}
  y2     <- c(rep(-1, n1), rep(1, n2))  # {-1, 1}
  # Generate dataframe
  data <- data.frame(rbind(class1, class2), y, y2)
  return(data)
}
# Generate reproducible data
set.seed(123)
data <- generate_binary_data(n1, n2, m1, s1, m2, s2)
X <- data %>% select(-y, -y2) %>% as.matrix
y <- data %>% select(y) %>% as.matrix
```
### Data Appearance
```{r}
plot_dataset <- function(data){
  p <- ggplot(data=data, aes(x=X1, y=X2, color=as_factor(y))) + 
        geom_point() + 
        theme(plot.title=element_text(hjust=0.5, size=20)) + 
        labs(color="Class", title="Linearly Separable Dataset")
  return(p)
}
plot_dataset(data)
```

# Fisher Discriminant Analysis
### FDA Class
Define an S3 object performing FDA.

```{r}
fda <- function(X, y){
  # Use y to create a logical one-hot encoding called `flags`
  flags <- make_flags(y)
  # Define objective function 
  fda_objective <- function(w){
    mu <- mean(X %*% w)           # embedded DATASET center
    muks <- rep(0, ncol(flags))   # embedded center for class k
    swks <- rep(0, ncol(flags))   # within class scatterness
    sbks <- rep(0, ncol(flags))   # between class scatterness
    for (class in 1:ncol(flags)){
      Xk <- X[flags[, class], ]
      mk <- mean(Xk %*% w)
      muks[class] <- mk
      swks[class] <- sum(((Xk %*% w) - mk)^2)
      sbks[class] <- sum(flags[, class]) * (mk - mu)^2
    }
    # Calculate objective value
    value <- sum(sbks) / sum(swks)
    return(-value) # remember we want to maximize, but optim minimizes
  }
  # Optimize
  w_start <- matrix(1, nrow=ncol(X), ncol=1)
  sol <- optim(par=w_start, fn=fda_objective, method="BFGS")$par
  # Return object
  fda_object <- list(sol=sol, flags=flags, X=X, y=y)
  class(fda_object) <- "FDA"
  return(fda_object)
}
```

### FDA Plot Method

```{r}
plot.FDA <- function(x, y=NULL, ...){
  # Find unit vector of w and take dot product
  sol_unit <- x$sol / sqrt(sum(x$sol^2))
  dot_products <-  x$X %*% sol_unit
  if (ncol(x$X) > 2){
    # Plot on a simple horizontal line
  df <- data.frame(x1=dot_products, x2=rep(0, nrow(dot_products)), y=x$y)
  p <- ggplot(data=df) + 
        geom_point(aes(x=x1, y=x2, color=as_factor(x$y)))
  } else {
    # Find embedded points in 2D
    x_emb <- dot_products %*% t(sol_unit)
    dfembed <- data.frame(x1=x_emb[, 1], x2=x_emb[, 2], y=x$y)
    # Find data mean, and mean per cluster
    datamean <- apply(x$X, 2, mean)
    datamean <- data.frame(x=datamean[1], y=datamean[2])
    meanmatrix <- calc_func_per_class(x$X, x$y, mean)
    dfclassmeans <- data.frame(meanmatrix, y=(1:nrow(meanmatrix) - 1))
    # Dataframe to plot w
    wdf <- data.frame(x=x$sol[1], y=x$sol[2], x0=c(0.0), y0=c(0.0))
    # Plot
    p <- ggplot() + 
          geom_point(data=data.frame(x$X, x$y), aes(x=X1, y=X2, color=as_factor(y)), alpha=0.2) + 
          geom_point(data=dfclassmeans, aes(x=X1, y=X2, color=as_factor(y)), shape=3, size=8, show.legend=FALSE) + 
          geom_point(data=datamean, aes(x=x, y=y), size=8, shape=3, color="black") +
          geom_point(data=dfembed, aes(x=x1, y=x2, color=as_factor(y))) + 
          geom_segment(data=wdf, aes(x=x0, y=y0, xend=x, yend=y, color="w"), 
                       arrow = arrow(length=unit(0.15, "inches")), color="darkred", size=1)
  }
  p + 
    labs(color="Class", title="FDA-Embedded Dataset", x="X1", y="X2") + 
    theme(plot.title=element_text(hjust=0.5, size=20))
}
```

### Results
```{r}
fda_object <- fda(X, y)
plot(fda_object)
```

# Naive Bayes
### Mathematics
$$
\widehat{y} = {\arg \max}_{k\in\left\{1, \ldots, K\right\}} p(C_k) \prod_{i=1}^N p(x_i \mid C_k)
$$
Let's assume each class is Gaussian distributed. 

### Naive Bayes Class
```{r}
naive_bayes <- function(X, y){
  # For every class, find mean and variance (one class per row)
  means <- calc_func_per_class(X, y, mean)
  vars  <- calc_func_per_class(X, y, function(x) sd(x)^2)
  # Create a gaussian for each class
  nb <- list(means=means, vars=vars, X=X, y=y)
  class(nb) <- "naive_bayes"
  return(nb)
}
```

### Predict Method
```{r}
predict.naive_bayes <- function(x, xtest){
  # for every test point (row of xtest), want to calculate the density value for
  # every class, and pick the highest one. Let's instantiate a matrix of the correct dimensions.
  table <- matrix(0, nrow=nrow(xtest), ncol=nrow(x$means))  # number of test points times n classes
  # need two for loops unfortunately
  for (r_ix in 1:nrow(xtest)){
    for (c_ix in 1:nrow(x$means)){
      table[r_ix, c_ix] <- gaussian_density(
        x=matrix(as.double(xtest[r_ix, ])), 
        mu=matrix(x$means[c_ix, ]), 
        sigma=diag(x$vars[c_ix, ]), log=TRUE)
    }
  }
  classes <- apply(table, 1, which.max)
  return(classes - 1)
}
```

### Plot Method for Naive Bayes
```{r}
plot.naive_bayes <- function(x, y=NULL, ngrid=75, ...){
  if (ncol(x$X) > 2) {
    print("Naive Bayes plotting for more than 2 dimensions, not implemented.")
  } else {
    # Generate a grid of points on X
    coord_range <- apply(x$X, 2, range)
    grid <- expand.grid(
      X1=seq(from=coord_range[1, 1], to=coord_range[2, 1], length=ngrid),
      X2=seq(from=coord_range[1, 2], to=coord_range[2, 2], length=ngrid)
      )
    # Use naive bayes to predict at each point of the grid
    cl <- predict(x, grid)
    dfgrid <- data.frame(grid, y=cl)
    # plot those points
    ggplot() + 
      geom_point(data=dfgrid, aes(x=X1, y=X2, color=as_factor(y)), alpha=0.2, size=0.5) + 
      geom_point(data=data.frame(x$X, y=x$y), aes(x=X1, y=X2, color=as_factor(y))) + 
      labs(color="Class", title="Naive Bayes Decision Boundary") + 
      theme(plot.title=element_text(hjust=0.5, size=20))
  }
}
```

### Results
```{r}
plot(naive_bayes(X, y))
```


# Logistic Regression
### Mathematical Setting
Let $Y_i\mid \vx_i \sim \text{Bernoulli}(p_i)$ with $p_i = \sigma(\vx_i^\top \vbeta)$ where $\sigma(\cdot)$ is the **sigmoid function**. The joint log-likelihood is given by
$$
\ln p(\vy\mid \vbeta) = \sum_{i=1}^n y_i \ln(p_i) + (1 - y_i)\ln(1 - p_i)
$$
### Maximum Likelihood Estimation
Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Minimizing the negative log likelihood is equivalent to solving the following optimization problem

$$
\min_{\vbeta}\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right)
$$

### Maximum-A-Posteriori and Ridge Regularization
We can introduce an isotropic Gaussian prior on **all** the coefficients $p(\vbeta) = N(\vzero, \sigma_{\vbeta}^2 I)$. Maximizing the posterior $p(\vbeta \mid \vy)$ is equivalent to minimizing the negative log posterior $-\ln p(\vbeta\mid \vy)$ giving

$$
\min_{\vbeta} \sigma^2_{\vbeta}\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right) + \frac{1}{2}\vbeta^\top\vbeta
$$
Often we don't want to regularize the intercept. For this reason we place an isotropic Gaussian prior on $\vbeta_{1:p-1}:=(\beta_1, \ldots, \beta_{p-1})$ and instead we place a uniform distribution on $\beta_0$, which doesn't depend on $\beta_0$. This leads to 
$$
\min_{\vbeta} \sigma_{\vbeta_{1:p-1}}^2\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right) + \frac{1}{2}\vbeta_{1:p-1}^\top\vbeta_{1:p-1}
$$

### Laplace Approximation
A fully-Bayesian treatment is intractable. Instead we approximate the posterior with a multivariate Gaussian distribution centered at the mode
$$
q(\vbeta) = N\left(\vbeta_{\text{MAP}}, \left[-\nabla^2 \ln p(\vbeta_{\text{MAP}}\mid \vy)\right]^{-1}\right)
$$
where one can show that the variance-covariance matrix 
$$
-\nabla_{\vbeta}^2 \ln p(\vbeta \mid \vy) = \vSigma_0^{-1} + \sum_{i=1}^n \sigma(\vx_i^\top \vbeta)(1 - \sigma(\vx_i^\top \vbeta)) \vx_i\vx_i^\top = \vSigma_0^{-1} + X^\top D X
$$
where
$$
D = 
\begin{pmatrix}
\sigma(\vx_1^\top\vbeta)(1 - \sigma(\vx_1^\top\vbeta)) & 0 & \ldots & 0 \\
0 & \sigma(\vx_2^\top\vbeta)(1 - \sigma(\vx_2^\top\vbeta)) & \ldots & 0 \\
\vdots & \ldots & \ddots & \vdots \\
0 & 0 & \ldots &\sigma(\vx_n^\top\vbeta)(1 - \sigma(\vx_n^\top\vbeta)) 
\end{pmatrix}
$$

### Gradient Ascent (MLE, No Regularization)
Updates take the form
$$
\vbeta_{k+1} \leftarrow \vbeta_k + \gamma_k X^\top(\vy - \sigma(X\vbeta_k))
$$
where the step size $\gamma_k$ can either be chosen small and constant or can be chosen using line search, leading to 
$$
\gamma_k = \frac{|(\vbeta_k - \vbeta_{k-1})^\top [\nabla \ln p(\vy \mid \vbeta_k) - \nabla\ln p(\vy\mid \vbeta_{k-1})]|}{\parallel\nabla\ln p(\vy\mid \vbeta_k) - \nabla \ln p(\vy\mid \vbeta_{k-1}\parallel^2}
$$

### Gradient Ascent (MAP, Ridge Regularization)
The update takes the form
$$
\vbeta_{k+1}\leftarrow  \vbeta_k + \gamma_k\left[\sigma_{\vbeta}^2X^\top(\vy - \sigma(X\vbeta_k)) - \vbeta_k\right]
$$

### Newton's Method (MLE, No Regularization)
The iterations are as follows, where for stability one can add a learning rate $\alpha$, which is in practice often set to $\alpha=0.1$.
$$
\vbeta_{k+1} \leftarrow \vbeta_k +  \alpha(X^\top D X)^{-1} X^\top(\vy - \sigma(X\vbeta_k))
$$
In practice we would solve the corresponding system for $\vect{d}$
$$
(X^\top D X)\vect{d}_k = \alpha X^\top(\vy - \sigma(X\vbeta_k))
$$
and then perform the update
$$
\vbeta_{k+1}\leftarrow \vbeta_k + \vect{d}_k
$$

### Newton's Method (MAP, Ridge Regularization)
The update takes the form
$$
\vbeta_{k+1} \leftarrow \vbeta_k +  \alpha(\sigma_{\vbeta}^2X^\top D X - I_n)^{-1}\left[\sigma^2_{\vbeta}X^\top(\vy - \sigma(X\vbeta_k)) - \vbeta_k\right]
$$

### Object-Oriented Implementation

```{r}
sigmoid <- function(x) 1.0 / (1.0 + exp(-x))
```

```{r}
grad_ascent <- function(beta, niter=100, gamma=0.001, cost="MLE", sigmab=1.0){
  if (cost=="MLE"){
    for (i in 1:niter) beta <- beta + gamma * t(X) %*% (y - sigmoid(X %*% beta))
  } else if (cost=="MAP"){
    for (i in 1:niter) beta <- beta + gamma*(sigmab^2*t(X) %*% (y - sigmoid(X %*% beta)) - beta)
  }
  return(beta)
}
```

```{r}
newton_method <- function(beta, niter=100, alpha=0.1, cost="MLE", sigmab=1.0){
  # Learning rate is suggested at 0.1. For 1.0 standard Newton method is recovered
  if (cost=="MLE"){
    for (i in 1:niter){
      D_k <- diag(drop(sigmoid(X%*%beta)*(1 - sigmoid(X%*%beta))))
      d_k <- solve(t(X)%*%D_k %*% X, alpha*t(X) %*% (y - sigmoid(X %*% beta)))
      beta <- beta + d_k
    }
  } else if (cost=="MAP"){
    n <- ncol(X)
    for (i in 1:niter){
      D_k <- diag(drop(sigmoid(X%*%beta)*(1 - sigmoid(X%*%beta))))
      d_k <- solve(
        sigmab^2*t(X)%*%D_k%*%X - diag(n),
        alpha*(sigmab^2*t(X)%*%(y - sigmoid(X %*% beta)) - beta)
      )
      beta <- beta + d_k
    }
  }
  return(beta)
}
```

```{r}
logistic_regression <- function(X, y, cost="MLE", method="BFGS", sigmab=1.0, niter=100,
                                alpha=0.1, gamma=0.001, laplace=FALSE){
  start <- matrix(0, nrow=ncol(X))
  # Define cost functions
  mle_cost <- function(beta) sum(log(1 + exp((1 - 2*y) * (X %*% beta))))
  map_cost <- function(beta) (sigmab^2)*mle_cost(beta) + 0.5*sum(beta^2)
  # Determine selected Cost Function
  if      (cost == "MLE") costfunc <- mle_cost
  else if (cost == "MAP") costfunc <- map_cost
  # Use selected method for selected cost function
  if      (method=="BFGS")   sol <- optim(par=start, fn=costfunc, method=method)$par
  else if (method=="GA")     sol <- grad_ascent(start, niter, gamma, cost, sigmab)
  else if (method=="NEWTON") sol <- newton_method(start, niter, alpha, cost, sigmab)
  # Laplace only works with MAP, not MLE
  # Can specify precision==TRUE in my builtin gaussian function
  first <- (sigmab^2)*diag(ncol(X))
  second <- t(X) %*% diag(drop(sigmoid(X %*% sol)*(1-sigmoid(X %*% sol)))) %*% X
  precision <- first + second
  # Build S3 object and return it
  lr <- list(X=X, y=y, beta=sol, cost=cost, method=method, prec=precision)
  class(lr) <- "logistic_regression"
  return(lr)
}
```

```{r}
print.logistic_regression <- function(x, ...){
  cat("S3 Object of Class logistic_regression.\n")
  cat("Cost Function:        ", x$cost, "\n")
  cat("Optimization Method:  ", x$method, "\n")
  cat("Solution:             ", x$beta, "\n")
}
```

```{r}
predict.logistic_regression <- function(x, xtest){
  # sigmoid gives probability of being in class 1. So will give (rounded) 1 to 1
  return(round(1.0 / (1.0 + exp(-xtest %*% x$beta))))
}
```


```{r}
plot.logistic_regression <- function(x, y=NULL, ngrid=70, ...){
  # Grid to show decision boundary 
  coord_range <- apply(x$X, 2, range)
  grid <- expand.grid(
      X1=seq(from=coord_range[1, 2], to=coord_range[2, 2], length=ngrid),
      X2=seq(from=coord_range[1, 3], to=coord_range[2, 3], length=ngrid)
  )
  # need to append 1 and transform to numeric, somehow it's losing it
  grid <- matrix(as.numeric(unlist(grid)), nrow=ngrid^2)
  # Predict at each point of the grid
  cl <- predict(x, cbind(1, grid))
  dfgrid <- data.frame(grid, y=cl)
  # plot those points
  p <- ggplot(data=dfgrid)
  if (!is.null(x$prec)) {
    # calculate confidence interval?
    sd <- sqrt(diag(solve(x$prec)))
    beta_min <- x$beta - sd
    beta_max <- x$beta + sd
    # get range. Each column is one dimension
    ranges <- apply(x$X, 2, range)
    # understand what are the x_1 limits, from the limits of the second coordinate x_2
    x_lims <- sort((ranges[, 3] * x$beta[3] + x$beta[1]) / (-x$beta[2]))
    x1_vals <- seq(from=x_lims[1], to=x_lims[2], length.out = 200)
    x2_vals <- -(x$beta[2]/x$beta[3])*x1_vals - (x$beta[1]/x$beta[3])
    # same but for confidence interval
    x_lims_min <- sort((ranges[, 3] * beta_min[3] + beta_min[1]) / (-beta_min[2]))
    x_lims_max <- sort((ranges[, 3] * beta_max[3] + beta_max[1]) / (-beta_max[2]))
    x_tot_range <- range(c(x_lims_min, x_lims_max))
    x1_vals_tot <- seq(from=x_tot_range[1], to=x_tot_range[2], length.out=200)
    #x2_vals_min <- -(beta_min[2]/beta_min[3])*x1_vals_tot- (beta_min[1]/beta_min[3])
    #x2_vals_max <- -(beta_max[2]/beta_max[3])*x1_vals_tot - (beta_max[1]/beta_max[3])
    x2_vals_min <- -(beta_min[2]/beta_min[3])*x1_vals_tot- (beta_max[1]/beta_min[3])
    x2_vals_max <- -(beta_max[2]/beta_max[3])*x1_vals_tot - (beta_min[1]/beta_max[3])
    #x1_vals_min <- seq(from=x_lims_min[1], to=x_lims_min[2], length.out = 200)
    #x1_vals_max <- seq(from=x_lims_max[1], to=x_lims_max[2], length.out = 200)
    #x2_vals_min <- -(beta_min[2]/beta_min[3])*x1_vals_min - (beta_min[1]/beta_min[3])
    #x2_vals_max <- -(beta_max[2]/beta_max[3])*x1_vals_max - (beta_max[1]/beta_max[3])
    dflaplace <- data.frame(x1=x1_vals, x2=x2_vals,
                            x1tot=x1_vals_tot,
                            x2min=x2_vals_min, x2max=x2_vals_max)
    p <- p +
      coord_cartesian(xlim=ranges[, 2], ylim=ranges[, 3])  +
      geom_ribbon(data=dflaplace, aes(x=x1tot, ymin=x2min, ymax=x2max), fill="grey80") +
      geom_line(data=dflaplace, aes(x=x1, y=x2)) 
  }
  p + 
    geom_point(data=dfgrid, aes(x=X1, y=X2, color=as_factor(y)), alpha=0.2, size=0.5) + 
    geom_point(data=data.frame(x$X, y=x$y), aes(x=X1, y=X2, color=as_factor(y))) + 
    labs(color="Class", title="Logistic Regression Decision Boundary") + 
    theme(plot.title=element_text(hjust=0.5, size=20))
}
```

```{r}
X <- cbind(1, X)
```


```{r}
lr_bfgs_mle <- logistic_regression(X, y, cost="MLE", method="BFGS")
lr_bfgs_map <- logistic_regression(X, y, cost="MAP", method="BFGS")
lr_ga_mle   <- logistic_regression(X, y, cost="MLE", method="GA", niter = 1000)
lr_ga_map   <- logistic_regression(X, y, cost="MAP", method="GA", niter = 1000)
lr_nm_mle   <- logistic_regression(X, y, cost="MLE", method="NEWTON")
lr_nm_map   <- logistic_regression(X, y, cost="MAP", method="NEWTON")
```

```{r}
print(lr_bfgs_mle)
print(lr_ga_mle)
print(lr_nm_mle)
print(lr_bfgs_map)
print(lr_ga_map)
print(lr_nm_map)
```

```{r}
to_plot <- logistic_regression(X, y, cost="MAP", method="BFGS", laplace = TRUE)
plot(to_plot)
```









































