---
title: "Optimization for Classification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(microbenchmark)
library(tidyverse)
```

# Utility functions

```{r}
kernel_matrix <- function(X, sigmasq, Y=NULL){
  if (is.null(Y)){
    Y <- X  # TODO: This might be risky for big matrices. How does R copy work? Maybe split the code into two?
  }
  n <- nrow(X)
  m <- nrow(Y)
  # Find three matrices above
  Xnorm <- matrix(apply(X^2, 1, sum), n, m)
  Ynorm <- matrix(apply(Y^2, 1, sum), n, m, byrow=TRUE)
  XY <- tcrossprod(X, Y)
  # TODO: In this case do we use median of XY? If we are using it for prediction
  # I think sigmasq should be found only from training set, not from testing set.
  # Therefore we should pass sigmasq as a parameter in this function, and should be calculated 
  # beforehand.
  return(exp(-(Xnorm - 2*XY + Ynorm) / (2*sigmasq)))
}

# Creates a logical one-hot encoding, assumes classes 0, .. , K-1
make_flags <- function(y){
  classes <- unique(y)
  flags <- data.frame(y=y) %>% 
            mutate(rn=row_number(), value=1) %>% 
            spread(y, value, fill=0) %>% 
            select(-rn) %>% as.matrix %>% as.logical %>%
            matrix(nrow=nrow(y)) 
  return(flags)
}
# Calculates a function (usually mean or sd) for all classes
calc_func_per_class <- function(X, y, func){
  flags <- make_flags(y)
  return(t(apply(flags, 2, function(f) apply(X[f, ], 2, func))))
}
# Calculates density of a gaussian
gaussian_density <- function(x, mu, sigma, log=FALSE){
  # Cholesky, L is upper triangular
  L <- chol(sigma)
  kernel <- -0.5*crossprod(forwardsolve(t(L), x - mu))
  value <- -length(x)*log(2*pi)/2 -sum(log(diag(L))) + kernel
  if (!log){
    value <- exp(value)
  }
  return(value[[1]])
}
```

# Binary Dataset Generation
### Settings for Data Creation
```{r}
# Settings
n1 <- 100
n2 <- 100
m1 <- c(6, 6)
m2 <- c(-1, 1)
s1 <- matrix(c(1, 0, 0, 10), nrow=2, ncol=2)
s2 <- matrix(c(1, 0, 0, 10), nrow=2, ncol=2)
# Set a seed if you want
set.seed(123)
```
### Data Generating Function
```{r}
generate_binary_data <- function(n1, n2, m1, s1, m2, s2, label1=1, label2=-1){
  # x1, x2 and y for both classes
  class1 <- mvrnorm(n1, m1, s1)
  class2 <- mvrnorm(n2, m2, s2)
  y      <- c(rep(label1, n1), rep(label2, n2))
  # Generate dataframe
  data <- data.frame(rbind(class1, class2), y)
  return(data)
}
data <- generate_binary_data(n1, n2, m1, s1, m2, s2)
```
### Data Appearance
```{r}
plot_dataset <- function(data){
  p <- ggplot(data=data, aes(x=X1, y=X2, color=as_factor(y))) + 
        geom_point() + 
        theme(plot.title=element_text(hjust=0.5, size=20)) + 
        labs(color="Class", title="Linearly Separable Dataset")
  return(p)
}
plot_dataset(data)
```

# Fisher Discriminant Analysis
### Data Generation
First, get the design matrix `X` and the response vector `y`.
```{r}
set.seed(123)
# We want it to be 0, 1
fda_data <- generate_binary_data(n1, n2, m1, s1, m2, s2, label1=0, label2=1)
X_fda <- fda_data %>% select(-y) %>% as.matrix
y_fda <- fda_data %>% select(y) %>% as.matrix
plot_dataset(fda_data)
```

### FDA Class
Define an S3 object performing FDA.

```{r}
fda <- function(X, y){
  # Use y to create a logical one-hot encoding called `flags`
  flags <- make_flags(y)
  # Define objective function 
  fda_objective <- function(w){
    mu <- mean(X %*% w)           # embedded DATASET center
    muks <- rep(0, ncol(flags))   # embedded center for class k
    swks <- rep(0, ncol(flags))   # within class scatterness
    sbks <- rep(0, ncol(flags))   # between class scatterness
    for (class in 1:ncol(flags)){
      Xk <- X[flags[, class], ]
      mk <- mean(Xk %*% w)
      muks[class] <- mk
      swks[class] <- sum(((Xk %*% w) - mk)^2)
      sbks[class] <- sum(flags[, class]) * (mk - mu)^2
    }
    # Calculate objective value
    value <- sum(sbks) / sum(swks)
    return(-value) # remember we want to maximize, but optim minimizes
  }
  # Optimize
  w_start <- matrix(1, nrow=ncol(X), ncol=1)
  sol <- optim(par=w_start, fn=fda_objective, method="BFGS")$par
  # Return object
  fda_object <- list(sol=sol, flags=flags, X=X, y=y)
  class(fda_object) <- "FDA"
  return(fda_object)
}
```

### FDA Plot Method

```{r}
plot.FDA <- function(x, y=NULL, ...){
  # Find unit vector of w and take dot product
  sol_unit <- x$sol / sqrt(sum(x$sol^2))
  dot_products <-  x$X %*% sol_unit
  if (ncol(x$X) > 2){
    # Plot on a simple horizontal line
  df <- data.frame(x1=dot_products, x2=rep(0, nrow(dot_products)), y=x$y)
  p <- ggplot(data=df) + 
        geom_point(aes(x=x1, y=x2, color=as_factor(y)))
  } else {
    # Find embedded points in 2D
    x_emb <- dot_products %*% t(sol_unit)
    dfembed <- data.frame(x1=x_emb[, 1], x2=x_emb[, 2], y=y_fda)
    # Find data mean, and mean per cluster
    datamean <- apply(x$X, 2, mean)
    datamean <- data.frame(x=datamean[1], y=datamean[2])
    meanmatrix <- calc_func_per_class(x$X, x$y, mean)
    dfclassmeans <- data.frame(meanmatrix, y=(1:nrow(meanmatrix) - 1))
    # Dataframe to plot w
    wdf <- data.frame(x=x$sol[1], y=x$sol[2], x0=c(0.0), y0=c(0.0))
    # Plot
    p <- ggplot() + 
          geom_point(data=data.frame(x$X, x$y), aes(x=X1, y=X2, color=as_factor(y)), alpha=0.2) + 
          geom_point(data=dfclassmeans, aes(x=X1, y=X2, color=as_factor(y)), shape=3, size=8, show.legend=FALSE) + 
          geom_point(data=datamean, aes(x=x, y=y), size=8, shape=3, color="black") +
          geom_point(data=dfembed, aes(x=x1, y=x2, color=as_factor(y))) + 
          geom_segment(data=wdf, aes(x=x0, y=y0, xend=x, yend=y, color="w"), 
                       arrow = arrow(length=unit(0.15, "inches")), color="darkred", size=1)
  }
  p + 
    labs(color="Class", title="FDA-Embedded Dataset", x="X1", y="X2") + 
    theme(plot.title=element_text(hjust=0.5, size=20))
}
```

We can see this in action as follows

```{r}
fda_object <- fda(X_fda, y_fda)
plot(fda_object)
```

# Naive Bayes
### Mathematics
$$
\widehat{y} = {\arg \max}_{k\in\left\{1, \ldots, K\right\}} p(C_k) \prod_{i=1}^N p(x_i \mid C_k)
$$
Let's assume each class is Gaussian distributed. 

### Naive Bayes Class
```{r}
naive_bayes <- function(X, y){
  # For every class, find mean and variance (one class per row)
  means <- calc_func_per_class(X, y, mean)
  vars  <- calc_func_per_class(X, y, function(x) sd(x)^2)
  # Create a gaussian for each class
  nb <- list(means=means, vars=vars, X=X, y=y)
  class(nb) <- "naive_bayes"
  return(nb)
}
```

### Predict Method
```{r}
predict.naive_bayes <- function(x, xtest){
  # for every test point (row of xtest), want to calculate the density value for
  # every class, and pick the highest one. Let's instantiate a matrix of the correct dimensions.
  table <- matrix(0, nrow=nrow(xtest), ncol=nrow(x$means))  # number of test points times n classes
  # need two for loops unfortunately
  for (r_ix in 1:nrow(xtest)){
    for (c_ix in 1:nrow(x$means)){
      table[r_ix, c_ix] <- gaussian_density(
        x=matrix(as.double(xtest[r_ix, ])), 
        mu=matrix(x$means[c_ix, ]), 
        sigma=diag(x$vars[c_ix, ]), log=TRUE)
    }
  }
  classes <- apply(table, 1, which.max)
  return(classes - 1)
}
```

### Plot Method for Naive Bayes
```{r}
plot.naive_bayes <- function(x, y=NULL, ngrid=75, ...){
  if (ncol(x$X) > 2) {
    print("Naive Bayes plotting for more than 2 dimensions, not implemented.")
  } else {
    # Generate a grid of points on X
    coord_range <- apply(x$X, 2, range)
    grid <- expand.grid(
      X1=seq(from=coord_range[1, 1], to=coord_range[2, 1], length=ngrid),
      X2=seq(from=coord_range[1, 2], to=coord_range[2, 2], length=ngrid)
      )
    # Use naive bayes to predict at each point of the grid
    cl <- predict(x, grid)
    dfgrid <- data.frame(grid, y=cl)
    # plot those points
    ggplot() + 
      geom_point(data=dfgrid, aes(x=X1, y=X2, color=as_factor(y)), alpha=0.2, size=0.5) + 
      geom_point(data=data.frame(x$X, y=x$y), aes(x=X1, y=X2, color=as_factor(y))) + 
      labs(color="Class", title="Naive Bayes Decision Boundary") + 
      theme(plot.title=element_text(hjust=0.5, size=20))
  }
}
```

Let's see this in action.

```{r}
nb <- naive_bayes(X_fda, y_fda)
plot(nb)
```


# Logistic Regression
### Mathematical Setting
USE SONG NOTATION. $y_i\in\{+1, -1\}$. Cost function is

$$
-log(p(\mathcal{D}\mid \beta) = \sum_{i=1}^n \log\left(1 + \exp\left(-y_i({\bf{x}_i^\top \beta})\right)\right)
$$
Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Therefore we want
$$
\min_{\beta} \sum_{i=1}^n \log\left(1 + \exp\left(-y_i({\bf{x}_i^\top \beta})\right)\right)
$$
Regularized (MAP with Gaussian) is
$$
\min_{\beta} \frac{1}{2} \beta^\top\beta + \sum_{i=1}^n \log\left(1 + \exp\left(-y_i({\bf{x}_i^\top \beta})\right)\right)
$$

### Implementation

```{r}
data <- generate_binary_data(n1, n2, m1, s1, m2, s2)  # 1, -1
X <- data %>% select(-y) %>% as.matrix %>% cbind(1, .)
y <- data %>% select(y) %>%  as.matrix
```

### Object-Oriented
```{r}
logistic_regression <- function(X, y, method="BFGS"){
  # X needs to have first column of 1s for bias term
  if (sum(X[, 1] == 1) != nrow(X)){
    print("The first column of X must have only 1s. Added a column.")
    X <- cbind(1, X)
  }
  # Maximum Likelihood Estimator. Assume y in {+1, -1}
  beta <- optim(
    par=rep(0, ncol(X)),
    fn=function(b) sum(log(exp(y * (X %*% b)) + 1)),
    method=method)$par
  # S3 Object
  lr <- list(X=X, y=y, method=method, beta=beta)
  class(lr) <- "logistic_regression"
  return(lr)
}
```

Define a `predict` method.
```{r}
predict.logistic_regression <- function(x, xtest){
  # predictions consist of probabilities. In this case we use a simple decision
  # rule: > 0.5 then it's class 1 
  # round transforms to prob=1, prob=0. Then 1 - ... to assign correct classes
  classes  <- round(1.0 / (1.0 + exp(xtest %*% x$beta)))
  classes[classes < 1] <- -1
  return(classes)
}
```

Define a plotting method.
```{r}
plot.logistic_regression <- function(x, y=NULL, ngrid=70, ...){
  # Grid to show decision boundary 
  coord_range <- apply(x$X, 2, range)
  grid <- expand.grid(
      X1=seq(from=coord_range[1, 2], to=coord_range[2, 2], length=ngrid),
      X2=seq(from=coord_range[1, 3], to=coord_range[2, 3], length=ngrid)
  )
  # need to append 1 and transform to numeric, somehow it's losing it
  grid <- matrix(as.numeric(unlist(grid)), nrow=ngrid^2)
  # Predict at each point of the grid
  cl <- predict(x, cbind(1, grid))
  dfgrid <- data.frame(grid, y=cl)
  # plot those points
  ggplot() + 
    geom_point(data=dfgrid, aes(x=X1, y=X2, color=as_factor(y)), alpha=0.2, size=0.5) + 
    geom_point(data=data.frame(x$X, y=x$y), aes(x=X1, y=X2, color=as_factor(y))) + 
    labs(color="Class", title="Logistic Regression Decision Boundary") + 
    theme(plot.title=element_text(hjust=0.5, size=20))
}
```

Let's see this in action.
```{r}
lr <- logistic_regression(X, y)
plot(lr)
```












































