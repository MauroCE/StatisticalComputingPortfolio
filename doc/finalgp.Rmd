---
title: "Gaussian Processes in R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## General Settings and Function Definitions
```{r message=FALSE, warning=FALSE}
library(MASS)
library(reshape2)
library(tidyverse)
# Number of data points for plotting & Bandwidth squared of the RBF kernel
ntest <- 50
sigmasq <- 1.0
# Number of training points, standard deviation of additive noise
ntrain <- 20
sigma.n <- 0.5
# Define number of samples for prior gp mvn and posterior gp mvn to take
nprior_samples <- 3
npost_samples <- 3
# Set a seed in order to change the code
#set.seed(12345)
```
Define a squared exponential kernel, a function to calculate kernel matrix K between two matrices, and the regression function that will be used for training.
```{r}
squared_exponential <- function(x, c, sigmasq){
  return(exp(-0.5*sum((x - c)^2) / sigmasq))
}
kernel_matrix <- function(X, Xstar, sigmasq){
  # compute the kernel matrix
  K <- apply(
    X=Xstar,
    MARGIN=1, 
    FUN=function(xstar_row) apply(
      X=X, 
      MARGIN=1, 
      FUN=squared_exponential, 
      xstar_row,
      sigmasq
      )
    )
  return(K)
}
compute_sigmasq <- function(X){
  # Compute distance matrix expanding the norm. Return its median
  Xcross <- tcrossprod(X)
  Xnorms <- matrix(diag(Xcross), nrow(X), nrow(X), byrow=TRUE)
  return(median(Xnorms - 2*Xcross + t(Xnorms)))
}
regression_function <- function(x){
    val <- (x+5)*(x+2)*(x)*(x-4)*(x-3)/10 + 2
  return(val)
}
```
## Sampling functions from the GP prior
Suppose that we want to predict the output of some values `x_predict`. If we haven't seen any data beforehand the functions sampled from the GP prior will be centered around $0$ and will be "random".
```{r}
xtest <- matrix(seq(-5,5, len=ntest))
Kss <- kernel_matrix(xtest, xtest, sigmasq=sigmasq)
```

Sample some functions, melt to put in long form to be able to plot
```{r}
dat <- data.frame(x=xtest, t(mvrnorm(nprior_samples, rep(0, length=ntest), Kss)))
names(dat) <- c("x", "sample 1", "sample 2", "sample 3")
dat <- melt(dat, id="x")
```
Finally plot it
```{r}
# Notice we use -2 and 2 because would be mean - 2*sd but sd in our case is
# the square root of the diagonal entries. If you look at K every diagonal
# element is 1
ggplot(data=dat, aes(x=x, y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2, ymax=2, fill="grey80") + 
  geom_line(aes(color=variable)) + 
  geom_abline(slope=0.0, intercept=0.0, lty=2) +
  scale_y_continuous(lim=c(-3,3), name="output, f(x)") +
  xlab("input, x")
```

## Sampling after observing data
Now we work with additive noise instead. We choose a sigma that represents the variance of the zero-mean noise added to each observation.
```{r}
xtrain <- matrix(runif(ntrain, min=-5, max=5))
ytrain <- regression_function(xtrain) + matrix(rnorm(ntrain, sd=sigma.n))
dftrain <- tibble(x=xtrain, y=ytrain)
```

The matrix way of obtaining the GP mean and the GP variance covariance matrix is
```{r}
Ks <- kernel_matrix(xtest, xtrain, sigmasq)
K  <- kernel_matrix(xtrain, xtrain, sigmasq) 
cov_xx_inv <- solve(K + sigma.n^2 * diag(ntrain))
Ef <- Ks %*% (cov_xx_inv %*% ytrain)
Cf <- Kss - Ks %*% cov_xx_inv %*% t(Ks)
```
Alternatively, we can do it sequentially
```{r}
# Create empty vectors for mean and variances (not covariances)
gpmean <- rep(0, ntest)
gpvar <- rep(0, ntest)
# Cholesky factor for K + sigma^2 * I
L <- chol(K + sigma.n^2*diag(ntrain))  ## Upper triangular
alpha <- backsolve(L, forwardsolve(t(L), ytrain))
# Loop through all testing points and compute mean and variance
for (test_index in 1:nrow(xtest)) {
  kstar <- matrix(Ks[test_index, ])
  # GP Mean at curret test point
  gpmean[test_index] <- t(kstar) %*% alpha
  # Gp variance at current test point
  v <- backsolve(L, kstar)
  gpvar[test_index] <- 1.0 - t(v) %*% v
}
```
Notice that we could use the Cholesky decomposition to find the GP mean and GP variance-covariance matrix at once as follows 
```{r}
gpmeannew <- Ks %*% alpha
gpvcov <- Kss - Ks %*% backsolve(L, forwardsolve(t(L), t(Ks)))
# Alternatively one can do this, which is much more similar to Cf
gpvcovnew <- Kss - Ks %*% chol2inv(L) %*% t(Ks)
```

Get 3 random samples and plot everything
```{r}
# the covariance matrix is useful to draw samples
dat <- data.frame(x=xtest, t(mvrnorm(npost_samples, gpmean, Cf)))
names(dat) <- c("x", "sample 1", "sample 2", "sample 3")
dat <- melt(dat, id="x")
# grab the cf standard deviations
Cfsd <- sqrt(diag(Cf))
# create dataframe for mean
dfmean <- data.frame(x=xtest, y=gpmean, 
                     ymin=gpmean-2*Cfsd, ymax=gpmean+2*Cfsd,
                     ytrue=regression_function(xtest))
# Plot
ggplot() + 
  geom_ribbon(data=dfmean, aes(x=xtest, ymin=ymin, ymax=ymax), fill="grey80") + 
  geom_line(data=dat, aes(x=x, y=value, color=variable)) +
  geom_line(data=dfmean, aes(x=xtest, y=Ef), size=1) + 
  geom_point(data=dftrain, aes(x=x, y=y), color='red') + 
  geom_line(data=dfmean, aes(x=x, y=ytrue), color='darkred', lty=2) + 
  xlab("input, x") + 
  ylab("output, f(x)")
```

## A complete GP function
```{r}
gp <- function(X, y, sd, Xtest){
  # X: training input
  # y: training output
  # sd: standard deviation of the additive noise
  # Use data to find bandwidth^2, kernel matrix K(X,X) and K(X, X*)
  sigmasq <- compute_sigmasq(X)
  K <- kernel_matrix(X, X, sigmasq = sigmasq)
  Ks <- kernel_matrix(Xtest, X, sigmasq=sigmasq)
  # Instantiate empty vectors containing GP mean and GP variances (no vcov)
  gpmean <- rep(0, ntest)
  gpvar <- rep(0, ntest)
  # Cholesky factor for K + sigma^2 * I
  L <- chol(K + sd^2*diag(nrow(X)))  # Upper triangular
  alpha <- backsolve(L, forwardsolve(t(L), y))
  # Loop through all test rows and compute mean and variance sequentially
  for (test_index in 1:nrow(Xtest)) {
    kstar <- matrix(Ks[test_index, ])
    # GP Mean at curret test point
    gpmean[test_index] <- t(kstar) %*% alpha
    # Gp variance at current test point
    v <- backsolve(L, kstar)
    gpvar[test_index] <- 1.0 - t(v) %*% v  # cause SE(x*,x*)=1.0
  }
  # return gpmean and gpvar
  return(list(gpmean, gpvar))
}
```

There's two functions to do the testing phase. The first one does it in one go, it can be used when we have the whole testing set at hand. The second one instead calculates the predicted mean and variance given only one testing example at a time.
```{r}
# Function that find gp mean and gp variance-covariance matrix in one go
gpparams <- function(Ks, Kss, alpha){
  # GP Mean in one batch
  gpmean <- Ks %*% alpha
  # GP variance-covariance matrix in one batch
  # alternatively, gpvcov <- Kss - ks %*% backsolve(L, forwardsolve(t(L), t(Ks)))
  gpvcov <- Kss - crossprod(forwardsolve(t(L), t(Ks)))
  return(list(gpmean, gpvcov))
}
# Function that can be used ONLINE
gponline <- function(xtest, xtrain, sigmasq, alpha){
  # Find kernel evaluation against all training points
  kstar <- kernel_matrix(matrix(xtest), xtrain, sigmasq)
  # GP mean for current test point
  gpmean <- t(kstar) %*% alpha
  # GP variance for current test point
  gpvar <- 1.0 - crossprod(forwardsolve(t(L), kstar))
  return(list(gpmean, gpvar))
}
```


