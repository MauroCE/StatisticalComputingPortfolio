---
title: "SM1"
output:
  html_document: 
    toc: yes
    toc_depth: 4
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Least Squares
First of all, we need to get the data from the website. Since the data is hosted as a `.data` file, we can use `read.table()` function in base R to read that file into a dataframe. Notice that the last column in the dataframe is just a boolean variable flagging whether that particular sample was used to train the model at page 38 of the corresponding book.
```{r}
# Get data from the weblink
library(readr)
url <- "https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data"
df <- read.table(url)[1:9]
head(df, 3)
```
We will use the columns `lcavol`, `lweight`, `age`, `lbph`, `svi`, `lcp`, `gleason` and `pgg45` as explanatory variables, while `lpsa` will be our target variable. We can therefore divide the data into two dataframes:
```{r}
# Recall X must have a feature in each row and an observation in each column
X <- t(as.matrix(df[1:8]))  # (d+1) * n
# We add an extra row of 1s for the bias
X <- rbind(X, rep(1, dim(X)[2]))
y <- t(as.vector(df$lpsa)) # 1 * n
```
Recall that in lectures the solution to linear regression least squares was given by
$${\bf{w}}_{LS} = (X X^\top)^{-1}X{\bf{y}}^\top$$
where $y=(y_1, \ldots, y_n)\in \mathbb{R}^{1\times n}$ is a **row vector** and $n$ is the number of observations. The matrix $X\in\mathbb{R}^{(d+1)\times n}$ is of the form 
$$
X = 
\begin{bmatrix}
 {\bf{x}}_1 & {\bf{x}}_2 & \ldots & {\bf{x}}_n\\ 
 1 & 1 & \ldots & 1
\end{bmatrix}
$$
where ${\bf{x}}_i \in \mathbb{R}^{d\times 1}$ are **column vectors**.
Notice that ${\bf{w}}_{LS}$ can be found by solving the linear system of equations 
$$A{\bf{w}}_{LS} = {\bf{b}}$$
where $A = XX^\top$ and ${\bf{b}} = X{\bf{y}}^\top$. Using this trick we can write a function that returns the solution to the least squares
```{r}
least_squares <- function(X, y){
  # X must have dim (d+1)*n where d = number of features, and n = number of observations
  # y must be a row vector of dim 1*n
  A <- X%*%t(X)
  b <- X%*%t(y)
  return(solve(A, b))
}
```
In this particular case we obtain
```{r}
least_squares(X, y)
```
Then we can evaluate $f({\bf{x}}, {\bf{w}}_{LS})$, our fitted solution as follows
```{r}
linear_ls <- function(x, w){
  # This function simply implements the linear version of f(x, w) = w^t * x
  # w will be returned as a column vector from least_squares().
  x <- as.matrix(x)
  w <- as.matrix(w)
  # Want this function to do both matrix multiplication and dot product,
  # so check for dimension and if necessary transpose
  if ((dim(x)[2] == 1) && (dim(x)[2] != dim(w)[1])) {
    x <- t(x)
  }
  return(x%*%w)
}
```
This function then can be used as follows

```{r}
# Use 1:9 as a new observation
linear_ls(1:dim(X)[1], least_squares(X, y))
```
## Cross Validation
The following function `cv` implements k-fold Cross Validation. It works by first stacking together `X` and `y` to create a new dataframe called `xy` that has dimension `n` times `d+2` (it is `d+2` rather than `d+1` because of the extra `y` column.). Next, the function shuffles the rows of the dataframe using the `sample()` function. 

At this point, we can use the `cut()` function to create flags that signal to which of the `k` groups each row of the dataframe belongs to. After this, we go through a loop where we perform least squares on all the data except the current group of rows, and we test the result exactly on that hold-out group. 

```{r}
# for testing purposes let's set a seed
cv <- function(X, y, k){
  # K-fold cross validation. For leave-one-out set k to num of observations.
  # X should be a (d+1)*n matrix with d = num of features, n = numb of samples.
  # y should be 1*n target vector.
  # Create a vector where errors will be stored. Length will be k
  errors <- rep(0, k)
  # First stack together X and y. y will be last row of X. Then transpose.
  xy <- t(rbind(X, y))
  # shuffle the rows
  xyshuffled <- xy[sample(nrow(xy)), ]
  # Create flags for the rows of xyshuffled to be divided into k folds
  folds <- cut(seq(1, nrow(xyshuffled)), breaks=k, labels=FALSE)
  # Go through each fold and calculate train and test stuff
  for(i in 1:k){
    # Find indeces of rows in the hold-out (test) group
    test_ind <- which(folds==i, arr.ind=TRUE)
    # Use such indeces to grab test data
    test_x <- xyshuffled[test_ind, -dim(xyshuffled)[2]]
    test_y <- xyshuffled[test_ind, dim(xyshuffled)[2]]
    # Use the remaining indeces to grab training data
    train_x <- xyshuffled[-test_ind, -dim(xyshuffled)[2]]
    train_y <- xyshuffled[-test_ind, dim(xyshuffled)[2]]
    # Now use train_x and train_y data to find the parameters. Recall that we want them
    # with num of observations as the second dimension
    w <- least_squares(t(train_x), t(train_y))
    # Now use these parameters to find the fitted value for the current test data
    f <- linear_ls(test_x, w)
    # Now compare the function value with test_y with a suitable error function.
    e <- sum((f - test_y)^2)
    # Add error to error list
    errors[i] <- e
  }
  # Finally return the average error
  return(mean(errors))
}
```
We can try this function using $10$ fold cross validation
```{r}
cv(X, y, 10)
```
and even **leave-one-out** cross validation
```{r}
cv(X, y, dim(X)[2])
```

## Removing Features
We can evaluate leave-one-out cross validation on the dataset where one feature has been removed each time. The code would look similar to
```{r}
# There's dim(X)[1] features if we count also the bias feature
errors <- rep(0, dim(X)[1])
# Remove one feature at a time
for (i in 1:dim(X)[1]){
  X <- X[-i, ]
  errors[i] <- cv(X, y, dim(X)[2])
}
errors
```
The error increases because we are missing information about one of the features. We can plot the errors as follows
```{r}
plot(1:length(errors), errors, xlab="Features", xaxt = "n", main="CV errors with feature dropping", ylab="CV errors")
labels <- names(df[1:8])
labels[9] <- "bias"
axis(1, at=1:length(errors), labels=labels)
```

## Homework 2
We want to generate points 
$$y_i = \exp(1.5x_i - 1) + \epsilon_i \qquad \text{where } \epsilon_i \sim N(0, 0.64) \quad \text{for } i = 1, \ldots, 200$$
Notice that we can sample from a normal distribution using the `rnorm()` function. We can then generate the `x_i` uniformly between `0` and `1`. 
```{r}
# Generato 200 samples from a N(0, 0.64)
e <- rnorm(n=200, mean=0, sd=0.8)
# Generate uniform x_i in [0, 1]
x <- runif(n=200, min=0, max=1)
# Calculate y_i
y <- t(exp(1.5*x - 1) + e)
# we need to generate the correct matrix with 1s at the end
x <- rbind(t(as.matrix(x)), rep(1, length(x)))
```
Now we re-write the least-squares algorithm so that it uses regularization.
```{r}
reg_ls <- function(X, y, lambda=0){
  # X has to be (n features, n obs). y has to be (1, n obs)
  X <- as.matrix(X)
  y <- as.matrix(y)
  # Use solve to find the solution, rather than inverting the matrix
  A <- X%*%t(X) + diag(lambda, nrow=nrow(X), ncol=nrow(X))
  b <- X%*%t(y)
  # Notice that it defaults to lambda=0 so this function can also be used for "unregularized ls".
  return(solve(A, b))
}
```
We can check this is the same by comparing the output of the previous least squares function
```{r}
ls_value <- least_squares(x, y)
rls_value <- reg_ls(x, y)
ls_value == rls_value
```

At this point we want to re-implement cross-validation in a more general way. Now it uses regularized least squares and takes lambda as a parameter so that cv can now be used to find the optimal regularization parameter.

```{r}
library(parallel)
library(MASS)
library(latex2exp)

cv_parallel <- function(X, y, k, lambda=0){
  # K-fold cross validation. For leave-one-out set k to num of observations.
  # X should be a (d+1)*n matrix with d = num of features, n = numb of samples.
  # y should be 1*n target vector.
  # Create a vector where errors will be stored. Length will be k
  errors <- rep(0, k)
  # First stack together X and y. y will be last row of X. Then transpose.
  xy <- t(rbind(X, y))
  # shuffle the rows
  xyshuffled <- xy[sample(nrow(xy)), ]
  # Create flags for the rows of xyshuffled to be divided into k folds
  folds <- cut(seq(1, nrow(xyshuffled)), breaks=k, labels=FALSE)
  # Go through each fold and calculate train and test stuff
  for(i in 1:k){
    # Find indeces of rows in the hold-out (test) group
    test_ind <- which(folds==i, arr.ind=TRUE)
    # Use such indeces to grab test data
    test_x <- xyshuffled[test_ind, -dim(xyshuffled)[2]]
    test_y <- xyshuffled[test_ind, dim(xyshuffled)[2]]
    # Use the remaining indeces to grab training data
    train_x <- xyshuffled[-test_ind, -dim(xyshuffled)[2]]
    train_y <- xyshuffled[-test_ind, dim(xyshuffled)[2]]
    # Now use train_x and train_y data to find the parameters. Recall that we want them
    # with num of observations as the second dimension
    w <- reg_ls(t(train_x), t(train_y), lambda=lambda)
    # Now use these parameters to find the fitted value for the current test data
    f <- linear_ls(test_x, w)
    # Now compare the function value with test_y with a suitable error function.
    e <- sum((f - test_y)^2)
    # Add error to error list
    errors[i] <- e
  }
  # Finally return the average error
  return(mean(errors))
}

```

Now we instantiate a set of lambdas and parallelize the operation to find the best lambda among those. 


```{r}
# Define a wrapper function with only one input parameter.
wrapper <- function(lambda){
  return(cv_parallel(x, y, k=ncol(y), lambda=lambda))
}
# Now create a list containing all the lambda configurations
params = 10^seq(from=-3, to=1, by=0.0625)
# Finally we parallelize everything
meanerrors <- mclapply(params, wrapper, mc.cores=2)
# Plot the result (requires install.packages("latex2exp"))
plot(log(params, 10), meanerrors, xlab=TeX('$\\log_{10}(\\lambda)$'), ylab=TeX('CV Error'), main=TeX('Change in CV Error due to regularization parameter'))
# Find the minimum 
min_index <- which.min(unlist(meanerrors))
points(log(params, 10)[min_index], meanerrors[min_index], col='red', pch=19)
legend(x='topleft', legend=c("Minimum CV Error"), col=c('red'), bg='white', lwd=1)
```

The minimum CV error is shown as a red point in the plot and corresponds to 
```{r}
params[min_index]
```

By using an optimization routine we can indeed see that this value is very close to the optimal one, which can be found like this

```{r}
sol <- optimize(wrapper, params)
sol$minimum
```

At this point we plot the predictive distribution. To do so, we write a wrapper function for the normal distribution, since the mean and the variance are fairly complicated functions

```{r}
predictive <- function(xhat, X, y, sigmasq, lambda=sol$minimum){
  # First find the mean. This is given by the prediction by the regularized least squares. In our case, we need to find 
  # W_LS-R with the current optimal value of lambda. To find it, we need to solve a system of linear equations
  X  <-  as.matrix(X)
  y <-  as.matrix(y)
  A <-  X%*%t(X) + diag(lambda, nrow=nrow(X), ncol=nrow(X))
  b <-  X%*%t(y)
  w_lsr <- solve(A, b)
  # notice xhat must be 2x1
  mean <- t(w_lsr)%*%xhat  
  # Next, find the variance
  variance <- sigmasq + sigmasq*((t(xhat)%*%solve(A))%*%xhat)
  return(c(mean, variance))
}
```

Obtain mean and variance for the predictive distribution

```{r}
xhat <- matrix(c(0.1, 1), 2, 1)
sigmasq <- 0.1
pred_params <- predictive(xhat, x, y, sigmasq)
```

and finally we can plot the mean of the predictive distribution as $\widehat{{\bf{x}}}$ changes.

```{r}

```


